{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323f1a22",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformer_lens sae_lens langdetect unbabel-comet seaborn sentence_transformers scipy--quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f8753e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import sys\n",
    "from scipy.stats import pearsonr\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sae_lens import (\n",
    "    SAE,\n",
    "    ActivationsStore,\n",
    "    HookedSAETransformer,\n",
    "    LanguageModelSAERunnerConfig,\n",
    "    SAEConfig,\n",
    "    SAETrainingRunner,\n",
    "    upload_saes_to_huggingface,\n",
    ")\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from langdetect import detect\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import networkx as nx\n",
    "from functools import partial\n",
    "import pickle\n",
    "from jaxtyping import Float, Int\n",
    "\n",
    "from typing import List\n",
    "from rich import print as rprint\n",
    "from rich.table import Table\n",
    "from huggingface_hub import notebook_login\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16cc8a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "gemma_9b: HookedSAETransformer = HookedSAETransformer.from_pretrained(\"gemma-2-9b\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232a1933",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "GENERATE_KWARGS = dict(temperature=0.5, freq_penalty=1.0, verbose=False)\n",
    "\n",
    "def steering_hook(\n",
    "    activations: Float[torch.Tensor, \"batch pos d_in\"],\n",
    "    hook: HookPoint,\n",
    "    sae: SAE,\n",
    "    latent_idx: int,\n",
    "    steering_coefficient: float,\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Steers the model by returning a modified activations tensor, with some multiple of the steering vector added to all\n",
    "    sequence positions.\n",
    "    \"\"\"\n",
    "    return activations + steering_coefficient * sae.W_dec[latent_idx]\n",
    "\n",
    "#USE RUN WITH CACHE WWITH SAE\n",
    "def generate_with_multi_steering(\n",
    "    model: HookedSAETransformer,\n",
    "    steer_saes: list[SAE],\n",
    "    latent_idxs: List[int],\n",
    "    steering_coefficients: List[float],\n",
    "    monitor_saes: list[SAE],\n",
    "    prompts: List[str],\n",
    "    max_new_tokens: int = 50,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates text with multiple steering vectors applied from different SAEs.\n",
    "    Each SAE modifies activations at its own hook point with its own latent and coefficient.\n",
    "    \"\"\"\n",
    "    assert len(steer_saes) == len(latent_idxs) == len(steering_coefficients), \"Mismatch in lengths of SAE-related lists.\"\n",
    "\n",
    "    # Create individual hook functions for each SAE\n",
    "    hooks = []\n",
    "    for sae, latent_idx, coeff in zip(steer_saes, latent_idxs, steering_coefficients):\n",
    "        hook_fn = partial(\n",
    "            steering_hook,\n",
    "            sae=sae,\n",
    "            latent_idx=latent_idx,\n",
    "            steering_coefficient=coeff,\n",
    "        )\n",
    "        hooks.append((sae.cfg.hook_name, hook_fn))\n",
    "\n",
    "    with model.hooks(fwd_hooks=hooks):\n",
    "        if monitor_saes:\n",
    "            _, cache = model.run_with_cache_with_saes(prompts, saes=monitor_saes)\n",
    "        output = model.generate(prompts, max_new_tokens=max_new_tokens, **GENERATE_KWARGS)\n",
    "    if monitor_saes:\n",
    "        monitor_activations = [cache[f\"{sae.cfg.hook_name}.hook_sae_acts_post\"] for sae in monitor_saes]\n",
    "        return output, monitor_activations\n",
    "    return output\n",
    "\n",
    "def generate_with_multi_steering_batch(\n",
    "    model,\n",
    "    steer_layers: List[int],\n",
    "    steer_feature_idx: List[int],\n",
    "    steer_feature_diff: List[float],\n",
    "    monitor_layers: List[int] = [],\n",
    "    prompts: List[str] = ['I was learning'],\n",
    "):\n",
    "    assert len(steer_layers) == len(steer_feature_idx) == len(steer_feature_diff), \"All input lists must be the same length.\"\n",
    "\n",
    "    # Load all required SAEs (one per layer)\n",
    "    steer_saes = []\n",
    "    for layer in steer_layers:\n",
    "        sae, cfg_dict_layer, sparsity_layer = SAE.from_pretrained(\n",
    "            release='gemma-scope-9b-pt-res-canonical',\n",
    "            sae_id=f\"layer_{layer}/width_16k/canonical\",\n",
    "            device=str(device),\n",
    "        )\n",
    "        steer_saes.append(sae)\n",
    "    monitor_saes = []\n",
    "    for layer in monitor_layers:\n",
    "        sae, cfg_dict_layer, sparsity_layer = SAE.from_pretrained(\n",
    "            release='gemma-scope-9b-pt-res-canonical',\n",
    "            sae_id=f\"layer_{layer}/width_16k/canonical\",\n",
    "            device=str(device),\n",
    "        )\n",
    "        monitor_saes.append(sae)\n",
    "    # Store all outputs\n",
    "    generated_sentences = {}\n",
    "\n",
    "    # No steering generation\n",
    "    generated_sentences[0] = model.generate(prompts, max_new_tokens=50, **GENERATE_KWARGS)\n",
    "\n",
    "    # Multi-steered generation\n",
    "    if monitor_layers:\n",
    "        steered_output, monitor_activations = generate_with_multi_steering(\n",
    "            model=model,\n",
    "            steer_saes=steer_saes,\n",
    "            latent_idxs=steer_feature_idx,\n",
    "            steering_coefficients=[diff for diff in steer_feature_diff],\n",
    "            monitor_saes = monitor_saes,\n",
    "            prompts=prompts,\n",
    "            max_new_tokens=50,\n",
    "        )\n",
    "        generated_sentences[1] = steered_output\n",
    "        return generated_sentences, monitor_activations\n",
    "    else:\n",
    "        steered_output = generate_with_multi_steering(\n",
    "            model=model,\n",
    "            steer_saes=steer_saes,\n",
    "            latent_idxs=steer_feature_idx,\n",
    "            steering_coefficients=[diff for diff in steer_feature_diff],\n",
    "            monitor_saes = monitor_saes,\n",
    "            prompts=prompts,\n",
    "            max_new_tokens=50,\n",
    "        )\n",
    "        generated_sentences[1] = steered_output\n",
    "        return generated_sentences\n",
    "\n",
    "def generate_with_steering_batch(\n",
    "    model,\n",
    "    steer_layer: int,\n",
    "    steer_feature_idxs: List[int],\n",
    "    steer_feature_diffs: List[float],\n",
    "    prompts: List[str] = ['I was learning'],\n",
    "):\n",
    "    sae, cfg_dict_layer, sparsity_layer = SAE.from_pretrained(\n",
    "            release='gemma-scope-9b-pt-res-canonical',\n",
    "            sae_id=f\"layer_{steer_layer}/width_16k/canonical\",\n",
    "            device=str(device),\n",
    "        )\n",
    "    generated_sentences = {}\n",
    "    generated_sentences[0] = model.generate(prompts, max_new_tokens=50, **GENERATE_KWARGS)\n",
    "    for i in range(len(steer_feature_idxs)):\n",
    "        steer_feature_idx = [steer_feature_idxs[i]]\n",
    "        steer_feature_diff = [steer_feature_diffs[i]]\n",
    "        steered_output = generate_with_multi_steering(\n",
    "            model=model,\n",
    "            steer_saes=[sae],\n",
    "            latent_idxs=steer_feature_idx,\n",
    "            steering_coefficients=[diff for diff in steer_feature_diff],\n",
    "            monitor_saes = [],\n",
    "            prompts=prompts,\n",
    "            max_new_tokens=50,\n",
    "        )\n",
    "        generated_sentences[i+1] = steered_output\n",
    "    return generated_sentences"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
